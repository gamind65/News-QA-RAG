{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM RAG News Article QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:48:43.737749Z",
     "iopub.status.busy": "2025-06-04T08:48:43.737515Z",
     "iopub.status.idle": "2025-06-04T08:48:43.744643Z",
     "shell.execute_reply": "2025-06-04T08:48:43.743786Z",
     "shell.execute_reply.started": "2025-06-04T08:48:43.737715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:48:43.745646Z",
     "iopub.status.busy": "2025-06-04T08:48:43.745405Z",
     "iopub.status.idle": "2025-06-04T08:49:53.810148Z",
     "shell.execute_reply": "2025-06-04T08:49:53.809414Z",
     "shell.execute_reply.started": "2025-06-04T08:48:43.745623Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [langchain-community]ngchain-community]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "pandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [langchain-ollama]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h>>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "############################################################################################# 100.0%                                      31.1%                                 31.9%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip -q\n",
    "!pip install -q langchain-community\n",
    "!pip install -q langchain-ollama\n",
    "!pip install -q faiss-cpu\n",
    "!curl -fsSL --no-progress-meter https://ollama.com/install.sh | sh\n",
    "# What -fsSL even mean??? -> https://kodekloud.com/community/t/curl-fssl-command-meaning/74713"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:49:53.812297Z",
     "iopub.status.busy": "2025-06-04T08:49:53.812061Z",
     "iopub.status.idle": "2025-06-04T08:49:53.817128Z",
     "shell.execute_reply": "2025-06-04T08:49:53.816256Z",
     "shell.execute_reply.started": "2025-06-04T08:49:53.812272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Kaggle run Ollama config only\n",
    "import subprocess\n",
    "process = subprocess.Popen('ollama serve; python3 python3 -c \"import warnings; warnings.filterwarnings(\"ignore\"); print(\"Done!\")\" ', shell=True) #runs on a different thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:49:53.818186Z",
     "iopub.status.busy": "2025-06-04T08:49:53.817933Z",
     "iopub.status.idle": "2025-06-04T08:49:56.027636Z",
     "shell.execute_reply": "2025-06-04T08:49:56.027088Z",
     "shell.execute_reply.started": "2025-06-04T08:49:53.818161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
      "Your new public key is: \n",
      "\n",
      "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAINl8USuxNWDLX6USuahDtbp5HDyV3cwrQzQRwFaQPt+V\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-06-04T08:49:53.842Z level=INFO source=routes.go:1234 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2025-06-04T08:49:53.842Z level=INFO source=images.go:479 msg=\"total blobs: 0\"\n",
      "time=2025-06-04T08:49:53.842Z level=INFO source=images.go:486 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-06-04T08:49:53.843Z level=INFO source=routes.go:1287 msg=\"Listening on 127.0.0.1:11434 (version 0.9.0)\"\n",
      "time=2025-06-04T08:49:53.843Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
      "time=2025-06-04T08:49:54.025Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-56f1f503-81d0-2965-472b-0f6a79849008 library=cuda variant=v12 compute=6.0 driver=12.6 name=\"Tesla P100-PCIE-16GB\" total=\"15.9 GiB\" available=\"15.6 GiB\"\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import ollama\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:49:56.028622Z",
     "iopub.status.busy": "2025-06-04T08:49:56.028275Z",
     "iopub.status.idle": "2025-06-04T08:49:56.243302Z",
     "shell.execute_reply": "2025-06-04T08:49:56.242514Z",
     "shell.execute_reply.started": "2025-06-04T08:49:56.028602Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>article</th>\n",
       "      <th>mint</th>\n",
       "      <th>lcsr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22887248</td>\n",
       "      <td>22884536</td>\n",
       "      <td>Không quân Mỹ ngày 30/7 cho biết 2 máy bay ném...</td>\n",
       "      <td>Ngày 30-7, Mỹ đã điều động 2 máy bay ném bom s...</td>\n",
       "      <td>0.620571</td>\n",
       "      <td>3.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23299990</td>\n",
       "      <td>23286469</td>\n",
       "      <td>Hai quả tên lửa Hyunmoo-2 đã được Hàn Quốc phó...</td>\n",
       "      <td>Chỉ 6 phút sau khi Triều Tiên bắn thử tên lửa ...</td>\n",
       "      <td>0.443295</td>\n",
       "      <td>3.951220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21868543</td>\n",
       "      <td>21868825</td>\n",
       "      <td>Các công tố viên Hàn Quốc sẽ đưa ra đề nghị bắ...</td>\n",
       "      <td>Ngày 27/3, các công tố viên Hàn Quốc đã xin lệ...</td>\n",
       "      <td>0.639676</td>\n",
       "      <td>3.490566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23803776</td>\n",
       "      <td>23805197</td>\n",
       "      <td>Ngày 3-11, thông tin từ Đồn Biên phòng Cửa Đại...</td>\n",
       "      <td>Thông tin từ Đồn Biên phòng Cửa Đại (Bộ đội Bi...</td>\n",
       "      <td>0.303665</td>\n",
       "      <td>3.422222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21240367</td>\n",
       "      <td>21249367</td>\n",
       "      <td>Một trẻ 10 ngày tuổi ở Pháp đã tử vong sau khi...</td>\n",
       "      <td>Bộ Y tế Pháp ngày 4/1 công bố ngừng bán Vitami...</td>\n",
       "      <td>0.310787</td>\n",
       "      <td>3.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23451141</td>\n",
       "      <td>23453164</td>\n",
       "      <td>Theo công ty mẹ của Yahoo, Verizon, vụ vi phạm...</td>\n",
       "      <td>Theo tiết lộ từ Verizon, hãng mua tài sản onli...</td>\n",
       "      <td>0.851073</td>\n",
       "      <td>3.351351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22342028</td>\n",
       "      <td>22342731</td>\n",
       "      <td>Truyền thông Nhật Bản đưa tin Chủ tịch Trung Q...</td>\n",
       "      <td>Truyền thông Nhật Bản ngày 22-5 đưa tin: Hồi t...</td>\n",
       "      <td>0.257584</td>\n",
       "      <td>4.489796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21662342</td>\n",
       "      <td>21657713</td>\n",
       "      <td>\"Tôi tin chắc rằng chuyến thăm Việt Nam của Nh...</td>\n",
       "      <td>Đây là chuyến thăm đầu tiên của Nhà vua và Hoà...</td>\n",
       "      <td>0.448921</td>\n",
       "      <td>3.819672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21311477</td>\n",
       "      <td>21311897</td>\n",
       "      <td>Choi Soon-sil, nhân vật trung tâm của vụ bê bố...</td>\n",
       "      <td>Bạn thân của Tổng thống Hàn Quốc, bà Choi Soon...</td>\n",
       "      <td>0.225991</td>\n",
       "      <td>3.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21509222</td>\n",
       "      <td>21496906</td>\n",
       "      <td>Cơ quan tình báo Mỹ và Hàn Quốc cho rằng Triều...</td>\n",
       "      <td>Triều Tiên được cho là sở hữu lượng nguyên liệ...</td>\n",
       "      <td>0.599324</td>\n",
       "      <td>3.212766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   summary_id  article_id                                            summary  \\\n",
       "0    22887248    22884536  Không quân Mỹ ngày 30/7 cho biết 2 máy bay ném...   \n",
       "1    23299990    23286469  Hai quả tên lửa Hyunmoo-2 đã được Hàn Quốc phó...   \n",
       "2    21868543    21868825  Các công tố viên Hàn Quốc sẽ đưa ra đề nghị bắ...   \n",
       "3    23803776    23805197  Ngày 3-11, thông tin từ Đồn Biên phòng Cửa Đại...   \n",
       "4    21240367    21249367  Một trẻ 10 ngày tuổi ở Pháp đã tử vong sau khi...   \n",
       "5    23451141    23453164  Theo công ty mẹ của Yahoo, Verizon, vụ vi phạm...   \n",
       "6    22342028    22342731  Truyền thông Nhật Bản đưa tin Chủ tịch Trung Q...   \n",
       "7    21662342    21657713  \"Tôi tin chắc rằng chuyến thăm Việt Nam của Nh...   \n",
       "8    21311477    21311897  Choi Soon-sil, nhân vật trung tâm của vụ bê bố...   \n",
       "9    21509222    21496906  Cơ quan tình báo Mỹ và Hàn Quốc cho rằng Triều...   \n",
       "\n",
       "                                             article      mint      lcsr  \n",
       "0  Ngày 30-7, Mỹ đã điều động 2 máy bay ném bom s...  0.620571  3.400000  \n",
       "1  Chỉ 6 phút sau khi Triều Tiên bắn thử tên lửa ...  0.443295  3.951220  \n",
       "2  Ngày 27/3, các công tố viên Hàn Quốc đã xin lệ...  0.639676  3.490566  \n",
       "3  Thông tin từ Đồn Biên phòng Cửa Đại (Bộ đội Bi...  0.303665  3.422222  \n",
       "4  Bộ Y tế Pháp ngày 4/1 công bố ngừng bán Vitami...  0.310787  3.956522  \n",
       "5  Theo tiết lộ từ Verizon, hãng mua tài sản onli...  0.851073  3.351351  \n",
       "6  Truyền thông Nhật Bản ngày 22-5 đưa tin: Hồi t...  0.257584  4.489796  \n",
       "7  Đây là chuyến thăm đầu tiên của Nhà vua và Hoà...  0.448921  3.819672  \n",
       "8  Bạn thân của Tổng thống Hàn Quốc, bà Choi Soon...  0.225991  3.884615  \n",
       "9  Triều Tiên được cho là sở hữu lượng nguyên liệ...  0.599324  3.212766  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_parquet('/kaggle/input/ds310-vietnamese-news-summary/train_data.parquet')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:49:56.244298Z",
     "iopub.status.busy": "2025-06-04T08:49:56.244079Z",
     "iopub.status.idle": "2025-06-04T08:49:56.250250Z",
     "shell.execute_reply": "2025-06-04T08:49:56.249546Z",
     "shell.execute_reply.started": "2025-06-04T08:49:56.244281Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bạn thân của Tổng thống Hàn Quốc, bà Choi Soon-sil, thừa nhận đã đưa ra ý kiến chỉnh sửa đối với một số bài phát biểu kể từ khi bà Park Geun-hye còn là ứng cử viên tổng thống.\n",
      "\n",
      "Hãng tin Yonhap dẫn Cơ quan Công tố Hàn Quốc cho biết bà Choi Soon-sil - nhân vật trung tâm của vụ bê bối chính trị hiện nay ở Hàn Quốc và là bạn thân lâu năm của Tổng thống Park Geun-hye, thừa nhận đã giúp bà Park chỉnh sửa nhiều bài phát biểu.\n",
      "Phía công tố đã trình lời nhận tội của bà Choi ra trước Tòa án Hiến pháp trong buổi điều trần thứ hai, trong đó bà Choi thừa nhận đã đưa ra ý kiến chỉnh sửa đối với một số bài phát biểu kể từ khi bà Park Geun-hye còn là ứng cử viên tổng thống.\n",
      "Một số ý kiến được chấp nhận và sử dụng. Tuy nhiên, bà Choi tuyên bố việc này là do bà hiểu rất rõ cách nghĩ của Tổng thống Park.\n",
      "Bà Choi Soon-sil đã bị bắt hồi tháng 11/2016 với những cáo buộc can thiệp vào công việc nhà nước trong khi không giữ bất kỳ vị trí nào trong chính phủ, cũng như lợi dụng mối quan hệ cá nhân với Tổng thống để gây quỹ bất hợp pháp từ nhiều doanh nghiệp và tập đoàn lớn Hàn Quốc.\n",
      "Đến nay, bà Choi vẫn bác bỏ hầu hết những cáo buộc này./.\n"
     ]
    }
   ],
   "source": [
    "article = str(data.iloc[8]['article'])\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:49:56.251274Z",
     "iopub.status.busy": "2025-06-04T08:49:56.251080Z",
     "iopub.status.idle": "2025-06-04T08:49:56.267138Z",
     "shell.execute_reply": "2025-06-04T08:49:56.266279Z",
     "shell.execute_reply.started": "2025-06-04T08:49:56.251259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks : 3\n",
      "Chunking time: 0.3027915954589844 ms\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "start = time.time()\n",
    "chunks = text_splitter.split_text(text=article)\n",
    "end = time.time() - start\n",
    "print(f\"Number of chunks : {len(chunks)}\")\n",
    "print(f\"Chunking time: {end*1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:49:56.268054Z",
     "iopub.status.busy": "2025-06-04T08:49:56.267849Z",
     "iopub.status.idle": "2025-06-04T08:49:56.282049Z",
     "shell.execute_reply": "2025-06-04T08:49:56.281369Z",
     "shell.execute_reply.started": "2025-06-04T08:49:56.268037Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0:\n",
      " Bạn thân của Tổng thống Hàn Quốc, bà Choi Soon-sil, thừa nhận đã đưa ra ý kiến chỉnh sửa đối với một số bài phát biểu kể từ khi bà Park Geun-hye còn là ứng cử viên tổng thống. \n",
      "\n",
      "\n",
      "Chunk 1:\n",
      " Hãng tin Yonhap dẫn Cơ quan Công tố Hàn Quốc cho biết bà Choi Soon-sil - nhân vật trung tâm của vụ bê bối chính trị hiện nay ở Hàn Quốc và là bạn thân lâu năm của Tổng thống Park Geun-hye, thừa nhận đã giúp bà Park chỉnh sửa nhiều bài phát biểu.\n",
      "Phía công tố đã trình lời nhận tội của bà Choi ra trước Tòa án Hiến pháp trong buổi điều trần thứ hai, trong đó bà Choi thừa nhận đã đưa ra ý kiến chỉnh sửa đối với một số bài phát biểu kể từ khi bà Park Geun-hye còn là ứng cử viên tổng thống. \n",
      "\n",
      "\n",
      "Chunk 2:\n",
      " Một số ý kiến được chấp nhận và sử dụng. Tuy nhiên, bà Choi tuyên bố việc này là do bà hiểu rất rõ cách nghĩ của Tổng thống Park.\n",
      "Bà Choi Soon-sil đã bị bắt hồi tháng 11/2016 với những cáo buộc can thiệp vào công việc nhà nước trong khi không giữ bất kỳ vị trí nào trong chính phủ, cũng như lợi dụng mối quan hệ cá nhân với Tổng thống để gây quỹ bất hợp pháp từ nhiều doanh nghiệp và tập đoàn lớn Hàn Quốc.\n",
      "Đến nay, bà Choi vẫn bác bỏ hầu hết những cáo buộc này./. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(chunks)):\n",
    "    print(f\"Chunk {i}:\\n\", chunks[i], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:49:56.284533Z",
     "iopub.status.busy": "2025-06-04T08:49:56.284298Z",
     "iopub.status.idle": "2025-06-04T08:49:56.296056Z",
     "shell.execute_reply": "2025-06-04T08:49:56.295386Z",
     "shell.execute_reply.started": "2025-06-04T08:49:56.284516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_vectordb(text_chunks):\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    vectordb = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:49:56.297111Z",
     "iopub.status.busy": "2025-06-04T08:49:56.296855Z",
     "iopub.status.idle": "2025-06-04T08:50:04.824849Z",
     "shell.execute_reply": "2025-06-04T08:50:04.824043Z",
     "shell.execute_reply.started": "2025-06-04T08:49:56.297089Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-06-04T08:49:57.045Z level=INFO source=download.go:177 msg=\"downloading 970aa74c0a90 in 3 100 MB part(s)\"\n",
      "time=2025-06-04T08:49:59.376Z level=INFO source=download.go:177 msg=\"downloading c71d239df917 in 1 11 KB part(s)\"\n",
      "time=2025-06-04T08:50:00.636Z level=INFO source=download.go:177 msg=\"downloading ce4a164fc046 in 1 17 B part(s)\"\n",
      "time=2025-06-04T08:50:01.889Z level=INFO source=download.go:177 msg=\"downloading 31df23ea7daa in 1 420 B part(s)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/06/04 - 08:50:03 | 200 |  7.494783626s |       127.0.0.1 | POST     \"/api/pull\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-06-04T08:50:03.911Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:03.912Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:03.912Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:03.912Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:04.026Z level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-56f1f503-81d0-2965-472b-0f6a79849008 parallel=1 available=16790978560 required=\"809.9 MiB\"\n",
      "time=2025-06-04T08:50:04.126Z level=INFO source=server.go:135 msg=\"system memory\" total=\"31.4 GiB\" free=\"29.9 GiB\" free_swap=\"0 B\"\n",
      "time=2025-06-04T08:50:04.127Z level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=13 layers.offload=13 layers.split=\"\" memory.available=\"[15.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"809.9 MiB\" memory.required.partial=\"809.9 MiB\" memory.required.kv=\"24.0 MiB\" memory.required.allocations=\"[809.9 MiB]\" memory.weights.total=\"260.9 MiB\" memory.weights.repeating=\"216.1 MiB\" memory.weights.nonrepeating=\"44.7 MiB\" memory.graph.full=\"48.0 MiB\" memory.graph.partial=\"48.0 MiB\"\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /root/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert\n",
      "llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5\n",
      "llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12\n",
      "llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768\n",
      "llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072\n",
      "llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false\n",
      "llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1\n",
      "llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000\n",
      "llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100\n",
      "llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103\n",
      "llama_model_loader: - type  f32:   51 tensors\n",
      "llama_model_loader: - type  f16:   61 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 260.86 MiB (16.00 BPW) \n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 5\n",
      "load: token to piece cache size = 0.2032 MB\n",
      "print_info: arch             = nomic-bert\n",
      "print_info: vocab_only       = 1\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 136.73 M\n",
      "print_info: general.name     = nomic-embed-text-v1.5\n",
      "print_info: vocab type       = WPM\n",
      "print_info: n_vocab          = 30522\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 101 '[CLS]'\n",
      "print_info: EOS token        = 102 '[SEP]'\n",
      "print_info: UNK token        = 100 '[UNK]'\n",
      "print_info: SEP token        = 102 '[SEP]'\n",
      "print_info: PAD token        = 0 '[PAD]'\n",
      "print_info: MASK token       = 103 '[MASK]'\n",
      "print_info: LF token         = 0 '[PAD]'\n",
      "print_info: EOG token        = 102 '[SEP]'\n",
      "print_info: max token length = 21\n",
      "llama_model_load: vocab only - skipping tensors\n",
      "time=2025-06-04T08:50:04.175Z level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /root/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 2 --parallel 1 --port 42425\"\n",
      "time=2025-06-04T08:50:04.175Z level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\n",
      "time=2025-06-04T08:50:04.175Z level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-06-04T08:50:04.176Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n",
      "time=2025-06-04T08:50:04.193Z level=INFO source=runner.go:815 msg=\"starting go runner\"\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\n",
      "load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so\n",
      "time=2025-06-04T08:50:04.292Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-06-04T08:50:04.295Z level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:42425\"\n",
      "llama_model_load_from_file_impl: using device CUDA0 (Tesla P100-PCIE-16GB) - 16013 MiB free\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /root/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert\n",
      "llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5\n",
      "llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12\n",
      "llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768\n",
      "llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072\n",
      "llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false\n",
      "llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1\n",
      "llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000\n",
      "llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100\n",
      "llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103\n",
      "llama_model_loader: - type  f32:   51 tensors\n",
      "llama_model_loader: - type  f16:   61 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 260.86 MiB (16.00 BPW) \n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 5\n",
      "load: token to piece cache size = 0.2032 MB\n",
      "print_info: arch             = nomic-bert\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 2048\n",
      "print_info: n_embd           = 768\n",
      "print_info: n_layer          = 12\n",
      "print_info: n_head           = 12\n",
      "print_info: n_head_kv        = 12\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 768\n",
      "print_info: n_embd_v_gqa     = 768\n",
      "print_info: f_norm_eps       = 1.0e-12\n",
      "print_info: f_norm_rms_eps   = 0.0e+00\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 3072\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 0\n",
      "print_info: pooling type     = 1\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 2048\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 137M\n",
      "print_info: model params     = 136.73 M\n",
      "print_info: general.name     = nomic-embed-text-v1.5\n",
      "print_info: vocab type       = WPM\n",
      "print_info: n_vocab          = 30522\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 101 '[CLS]'\n",
      "print_info: EOS token        = 102 '[SEP]'\n",
      "print_info: UNK token        = 100 '[UNK]'\n",
      "print_info: SEP token        = 102 '[SEP]'\n",
      "print_info: PAD token        = 0 '[PAD]'\n",
      "print_info: MASK token       = 103 '[MASK]'\n",
      "print_info: LF token         = 0 '[PAD]'\n",
      "print_info: EOG token        = 102 '[SEP]'\n",
      "print_info: max token length = 21\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "time=2025-06-04T08:50:04.427Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "load_tensors: offloading 12 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 13/13 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =   216.14 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =    44.72 MiB\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 8192\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 0\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (8192) > n_ctx_train (2048) -- possible training context overflow\n",
      "llama_context:  CUDA_Host  output buffer size =     0.00 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/06/04 - 08:50:04 | 200 |  885.221796ms |       127.0.0.1 | POST     \"/api/embed\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-06-04T08:50:04.678Z level=INFO source=server.go:630 msg=\"llama runner started in 0.50 seconds\"\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n"
     ]
    }
   ],
   "source": [
    "ollama.pull(\"nomic-embed-text\")\n",
    "vectordb = get_vectordb(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LLM and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:50:04.825810Z",
     "iopub.status.busy": "2025-06-04T08:50:04.825584Z",
     "iopub.status.idle": "2025-06-04T08:50:18.118049Z",
     "shell.execute_reply": "2025-06-04T08:50:18.117409Z",
     "shell.execute_reply.started": "2025-06-04T08:50:04.825794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-06-04T08:50:05.386Z level=INFO source=download.go:177 msg=\"downloading 3d0b790534fe in 14 100 MB part(s)\"\n",
      "time=2025-06-04T08:50:08.663Z level=INFO source=download.go:177 msg=\"downloading ae370d884f10 in 1 1.7 KB part(s)\"\n",
      "time=2025-06-04T08:50:09.913Z level=INFO source=download.go:177 msg=\"downloading d18a5cc71b84 in 1 11 KB part(s)\"\n",
      "time=2025-06-04T08:50:11.173Z level=INFO source=download.go:177 msg=\"downloading cff3f395ef37 in 1 120 B part(s)\"\n",
      "time=2025-06-04T08:50:12.453Z level=INFO source=download.go:177 msg=\"downloading 517ccaff02fe in 1 487 B part(s)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/06/04 - 08:50:18 | 200 | 13.187129161s |       127.0.0.1 | POST     \"/api/pull\"\n"
     ]
    }
   ],
   "source": [
    "# Set up LLM and retrieval\n",
    "ollama.pull(\"qwen3:1.7b\")\n",
    "local_model = \"qwen3:1.7b\"  \n",
    "llm = ChatOllama(model=local_model, enable_thinking=False, Temperature=0.7, TopP=0.8, TopK=20, MinP=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:50:18.119282Z",
     "iopub.status.busy": "2025-06-04T08:50:18.118810Z",
     "iopub.status.idle": "2025-06-04T08:50:18.123838Z",
     "shell.execute_reply": "2025-06-04T08:50:18.123289Z",
     "shell.execute_reply.started": "2025-06-04T08:50:18.119265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Query prompt template\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "    Bạn là một trợ lý với nhiệm vụ giúp đỡ người dùng trả lời những câu hỏi liên quan đến bài báo.\n",
    "    Bạn sẽ thực hiện điều này bằng cách tạo ra thêm 2 câu hỏi khác tương đồng với câu hỏi gốc để \n",
    "    tím kiếm những thông tin liên quan trong Vector Database.\n",
    "    Bằng cách tạo ra thêm những góc mới tương đồng với câu hỏi của người dùng, mục tiêu của bạn là \n",
    "    giúp người dùng vượt qua rào cản của việc tìm kiếm thông tin dựa trên khoảng cách tương đối về \n",
    "    mặt ngữ nghĩa của chúng. \n",
    "    Hãy cố gắng để không trả lời những thông tin tồn tại trong bài báo.\n",
    "    Nếu bạn không biết câu trả lời, hãy cứ trả lời là không biết.\n",
    "    Hãy đưa ra câu trả lời trên một dòng mới và đừng lặp lại câu hỏi của người dùng. \n",
    "    Hãy đưa ra câu trả lời một cách ngắn gọn.\n",
    "    Câu hỏi: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Set up retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vectordb.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:50:18.124807Z",
     "iopub.status.busy": "2025-06-04T08:50:18.124550Z",
     "iopub.status.idle": "2025-06-04T08:50:22.062736Z",
     "shell.execute_reply": "2025-06-04T08:50:22.061798Z",
     "shell.execute_reply.started": "2025-06-04T08:50:18.124785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# RAG prompt template\n",
    "template = \"\"\"Hãy trả lời câu hỏi mà CHỈ dựa vào thông tin được cho như sau:\n",
    "{context}\n",
    "Câu hỏi: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:50:22.063907Z",
     "iopub.status.busy": "2025-06-04T08:50:22.063661Z",
     "iopub.status.idle": "2025-06-04T08:50:22.076899Z",
     "shell.execute_reply": "2025-06-04T08:50:22.076123Z",
     "shell.execute_reply.started": "2025-06-04T08:50:22.063884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Article QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:50:22.077858Z",
     "iopub.status.busy": "2025-06-04T08:50:22.077636Z",
     "iopub.status.idle": "2025-06-04T08:50:22.091863Z",
     "shell.execute_reply": "2025-06-04T08:50:22.091060Z",
     "shell.execute_reply.started": "2025-06-04T08:50:22.077842Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def news_qa(question):\n",
    "    return display(Markdown(re.sub('<think>(?s:.)*?</think>', '', chain.invoke(question))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:50:22.092875Z",
     "iopub.status.busy": "2025-06-04T08:50:22.092658Z",
     "iopub.status.idle": "2025-06-04T08:50:32.711662Z",
     "shell.execute_reply": "2025-06-04T08:50:32.711070Z",
     "shell.execute_reply.started": "2025-06-04T08:50:22.092850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-06-04T08:50:22.287Z level=INFO source=sched.go:548 msg=\"updated VRAM based on existing loaded models\" gpu=GPU-56f1f503-81d0-2965-472b-0f6a79849008 library=cuda total=\"15.9 GiB\" available=\"15.1 GiB\"\n",
      "time=2025-06-04T08:50:22.288Z level=INFO source=sched.go:788 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-3d0b790534fe4b79525fc3692950408dca41171676ed7e21db57af5c65ef6ab6 gpu=GPU-56f1f503-81d0-2965-472b-0f6a79849008 parallel=2 available=16210342912 required=\"2.8 GiB\"\n",
      "time=2025-06-04T08:50:22.394Z level=INFO source=server.go:135 msg=\"system memory\" total=\"31.4 GiB\" free=\"29.6 GiB\" free_swap=\"0 B\"\n",
      "time=2025-06-04T08:50:22.395Z level=INFO source=server.go:168 msg=offload library=cuda layers.requested=-1 layers.model=29 layers.offload=29 layers.split=\"\" memory.available=\"[15.1 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"2.8 GiB\" memory.required.partial=\"2.8 GiB\" memory.required.kv=\"896.0 MiB\" memory.required.allocations=\"[2.8 GiB]\" memory.weights.total=\"1.1 GiB\" memory.weights.repeating=\"880.3 MiB\" memory.weights.nonrepeating=\"243.4 MiB\" memory.graph.full=\"298.7 MiB\" memory.graph.partial=\"298.7 MiB\"\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 311 tensors from /root/.ollama/models/blobs/sha256-3d0b790534fe4b79525fc3692950408dca41171676ed7e21db57af5c65ef6ab6 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen3 1.7B\n",
      "llama_model_loader: - kv   3:                           general.basename str              = Qwen3\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   5:                          qwen3.block_count u32              = 28\n",
      "llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\n",
      "llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 6144\n",
      "llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  26:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:  113 tensors\n",
      "llama_model_loader: - type  f16:   28 tensors\n",
      "llama_model_loader: - type q4_K:  155 tensors\n",
      "llama_model_loader: - type q6_K:   15 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.26 GiB (5.33 BPW) \n",
      "load: special tokens cache size = 26\n",
      "load: token to piece cache size = 0.9311 MB\n",
      "print_info: arch             = qwen3\n",
      "print_info: vocab_only       = 1\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 2.03 B\n",
      "print_info: general.name     = Qwen3 1.7B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "llama_model_load: vocab only - skipping tensors\n",
      "time=2025-06-04T08:50:22.688Z level=INFO source=server.go:431 msg=\"starting llama server\" cmd=\"/usr/local/bin/ollama runner --model /root/.ollama/models/blobs/sha256-3d0b790534fe4b79525fc3692950408dca41171676ed7e21db57af5c65ef6ab6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 29 --threads 2 --parallel 2 --port 43731\"\n",
      "time=2025-06-04T08:50:22.689Z level=INFO source=sched.go:483 msg=\"loaded runners\" count=2\n",
      "time=2025-06-04T08:50:22.689Z level=INFO source=server.go:591 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-06-04T08:50:22.689Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n",
      "time=2025-06-04T08:50:22.708Z level=INFO source=runner.go:815 msg=\"starting go runner\"\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\n",
      "load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v12/libggml-cuda.so\n",
      "time=2025-06-04T08:50:22.803Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-06-04T08:50:22.807Z level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:43731\"\n",
      "llama_model_load_from_file_impl: using device CUDA0 (Tesla P100-PCIE-16GB) - 15513 MiB free\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 311 tensors from /root/.ollama/models/blobs/sha256-3d0b790534fe4b79525fc3692950408dca41171676ed7e21db57af5c65ef6ab6 (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen3 1.7B\n",
      "llama_model_loader: - kv   3:                           general.basename str              = Qwen3\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   5:                          qwen3.block_count u32              = 28\n",
      "llama_model_loader: - kv   6:                       qwen3.context_length u32              = 40960\n",
      "llama_model_loader: - kv   7:                     qwen3.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   8:                  qwen3.feed_forward_length u32              = 6144\n",
      "llama_model_loader: - kv   9:                 qwen3.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  10:              qwen3.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  11:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  12:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  13:                 qwen3.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  14:               qwen3.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
      "time=2025-06-04T08:50:22.941Z level=INFO source=server.go:625 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  26:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:  113 tensors\n",
      "llama_model_loader: - type  f16:   28 tensors\n",
      "llama_model_loader: - type q4_K:  155 tensors\n",
      "llama_model_loader: - type q6_K:   15 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.26 GiB (5.33 BPW) \n",
      "load: special tokens cache size = 26\n",
      "load: token to piece cache size = 0.9311 MB\n",
      "print_info: arch             = qwen3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 40960\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 6144\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 40960\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1.7B\n",
      "print_info: model params     = 2.03 B\n",
      "print_info: general.name     = Qwen3 1.7B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: offloading 28 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 29/29 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  1123.71 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   166.92 MiB\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 2\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 1024\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
      "llama_context:  CUDA_Host  output buffer size =     1.17 MiB\n",
      "llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
      "llama_context:      CUDA0 compute buffer size =   300.75 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    20.01 MiB\n",
      "llama_context: graph nodes  = 1070\n",
      "llama_context: graph splits = 2\n",
      "time=2025-06-04T08:50:23.944Z level=INFO source=server.go:630 msg=\"llama runner started in 1.26 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/06/04 - 08:50:27 | 200 |  5.849706213s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/06/04 - 08:50:27 | 200 |   27.115521ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:28 | 200 |   27.697671ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:28 | 200 |   24.386276ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:28 | 200 |   27.596366ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:28 | 200 |   25.290362ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:28 | 200 |   27.152925ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:28 | 200 |   22.394134ms |       127.0.0.1 | POST     \"/api/embed\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-06-04T08:50:27.974Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:27.974Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:27.974Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:27.974Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:28.005Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:28.005Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:28.005Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:28.005Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:28.035Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:28.035Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:28.035Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:28.035Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:28.061Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:28.061Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:28.061Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:28.061Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:28.091Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:28.091Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:28.091Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:28.091Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:28.116Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:28.116Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:28.116Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:28.116Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:28.144Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:28.144Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:28.144Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:28.144Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:28.170Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:28.170Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:28.170Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:28.170Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:28.195Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:28.195Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:28.195Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:28.195Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:28.218Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:28.218Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:28.218Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:28.218Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/06/04 - 08:50:28 | 200 |   24.252665ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:28 | 200 |   22.663472ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:28 | 200 |   21.837231ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:32 | 200 |  4.467291706s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Bài báo tóm tắt rằng bà Choi Soon-sil, bạn thân của Tổng thống Hàn Quốc Park Geun-hye, bị điều tra về việc can thiệp vào công việc nhà nước và lợi dụng mối quan hệ cá nhân để gây quỹ bất hợp pháp. Bà đã bác bỏ cáo buộc, nhưng Cơ quan Công tố Hàn Quốc đã trình lời nhận tội của bà trước Tòa án Hiến pháp, xác nhận bà đã giúp Park chỉnh sửa nhiều bài phát biểu khi cô còn là ứng cử viên tổng thống. Vụ bê bối đang thu hút sự chú ý của dư luận."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize the article\n",
    "news_qa(\"Hãy tóm tắt bài báo này.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:50:32.713096Z",
     "iopub.status.busy": "2025-06-04T08:50:32.712456Z",
     "iopub.status.idle": "2025-06-04T08:50:39.760074Z",
     "shell.execute_reply": "2025-06-04T08:50:39.759517Z",
     "shell.execute_reply.started": "2025-06-04T08:50:32.713078Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/06/04 - 08:50:35 | 200 |  2.661588566s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/06/04 - 08:50:35 | 200 |   21.819084ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:35 | 200 |   25.768043ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:35 | 200 |   29.897245ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:35 | 200 |   26.009813ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:35 | 200 |   26.286557ms |       127.0.0.1 | POST     \"/api/embed\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-06-04T08:50:35.385Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:35.385Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:35.385Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:35.385Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:35.409Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:35.409Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:35.409Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:35.409Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:35.438Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:35.438Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:35.438Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:35.438Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:35.471Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:35.471Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:35.471Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:35.471Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:35.498Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:35.498Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:35.498Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:35.498Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/06/04 - 08:50:39 | 200 |  4.234333225s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Sự kiện đã diễn ra vào tháng 11/2016 là **bắt giữ bà Choi Soon-sil** vì những cáo buộc liên quan đến việc can thiệp vào công việc nhà nước và lợi dụng mối quan hệ với Tổng thống Park Geun-hye để gây quỹ bất hợp pháp."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ask about relevant information within article\n",
    "news_qa(\"Sự kiện gì đã diễn ra vào tháng 11/2016?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:50:39.761336Z",
     "iopub.status.busy": "2025-06-04T08:50:39.760861Z",
     "iopub.status.idle": "2025-06-04T08:50:48.131530Z",
     "shell.execute_reply": "2025-06-04T08:50:48.130802Z",
     "shell.execute_reply.started": "2025-06-04T08:50:39.761316Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/06/04 - 08:50:43 | 200 |  3.471418581s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/06/04 - 08:50:43 | 200 |   24.650463ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:43 | 200 |   28.684486ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:43 | 200 |   31.472128ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:43 | 200 |   24.198324ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:43 | 200 |   26.969994ms |       127.0.0.1 | POST     \"/api/embed\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-06-04T08:50:43.246Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:43.246Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:43.246Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:43.246Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:43.271Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:43.271Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:43.271Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:43.271Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:43.300Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:43.300Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:43.300Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:43.300Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:43.336Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:43.336Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:43.336Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:43.336Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:43.359Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:43.359Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:43.359Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:43.359Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/06/04 - 08:50:48 | 200 |  4.742097463s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Không có thông tin nào liên quan đến Lý Tự Trọng trong các bài báo được cung cấp. Câu hỏi này dựa trên thông tin không tồn tại trong văn bản đã cho."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ask about what did not happened in the given article\n",
    "news_qa(\"Lý Tự Trọng đã làm gì trong bài báo này?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T08:50:48.132610Z",
     "iopub.status.busy": "2025-06-04T08:50:48.132373Z",
     "iopub.status.idle": "2025-06-04T08:51:00.836764Z",
     "shell.execute_reply": "2025-06-04T08:51:00.836032Z",
     "shell.execute_reply.started": "2025-06-04T08:50:48.132594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/06/04 - 08:50:54 | 200 |   6.30135842s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "[GIN] 2025/06/04 - 08:50:54 | 200 |   24.172522ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:54 | 200 |   26.841305ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:54 | 200 |   27.680969ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:54 | 200 |    23.53735ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:54 | 200 |   23.350411ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:54 | 200 |   28.501822ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:54 | 200 |   24.685366ms |       127.0.0.1 | POST     \"/api/embed\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-06-04T08:50:54.446Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:54.446Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:54.446Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:54.446Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:54.470Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:54.470Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:54.470Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:54.470Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:54.499Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:54.499Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:54.499Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:54.499Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:54.528Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:54.528Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:54.528Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:54.528Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:54.555Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:54.555Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:54.555Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:54.555Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:54.582Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:54.582Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:54.582Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:54.582Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:54.608Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:54.608Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:54.608Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:54.608Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:54.635Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:54.635Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:54.635Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:54.635Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/06/04 - 08:50:54 | 200 |   25.136782ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:54 | 200 |    24.21228ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:54 | 200 |   22.812487ms |       127.0.0.1 | POST     \"/api/embed\"\n",
      "[GIN] 2025/06/04 - 08:50:54 | 200 |   22.469496ms |       127.0.0.1 | POST     \"/api/embed\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-06-04T08:50:54.661Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:54.661Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:54.662Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:54.662Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:54.689Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:54.689Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "time=2025-06-04T08:50:54.689Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:54.689Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n",
      "time=2025-06-04T08:50:54.711Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat\n",
      "time=2025-06-04T08:50:54.712Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_tau\n",
      "time=2025-06-04T08:50:54.712Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=tfs_z\n",
      "time=2025-06-04T08:50:54.712Z level=WARN source=types.go:572 msg=\"invalid option provided\" option=mirostat_eta\n",
      "decode: cannot decode batches with this context (use llama_encode() instead)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/06/04 - 08:51:00 | 200 |  6.099154784s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Câu hỏi trước có thể là: **\"Choi Soon-sil đã thừa nhận điều gì trong vụ bê bối chính trị tại Hàn Quốc?\"**  \n",
       "Hoặc một câu hỏi liên quan đến vai trò của bà Choi trong scandal, như: **\"Bà Choi Soon-sil có liên quan gì đến việc chỉnh sửa bài phát biểu của Tổng thống Park Geun-hye?\"**  \n",
       "**Lý do:** Document đã nêu rõ Choi thừa nhận đã chỉnh sửa một số bài phát biểu của Park khi cô còn là ứng cử viên tổng thống, và bà là bạn thân của Tổng thống."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Chat Bot conversation memory\n",
    "news_qa(\"Câu hỏi trước của tôi là gì?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook is about experimenting and implementing RAG for small scale LLMs with under **3 billion parameters** due to my limitation of hardware.  \n",
    "\n",
    "After implemented **Vector Database** for storing news article's chunk of embeddings to prevent overly long sequence of texts; and **RAG** to retrieve these embeddings effectively, LLMs now can answer questions about given article precisely.  \n",
    "  \n",
    "I have also tested on **gemma3-1b** but this model appear to hallucinate when i ask it about Ly Tu Trong (see *Ask about what not happened in the article*). It always associate an action happened in the article with Ly Tu Trong, which is not true.  \n",
    "\n",
    "**Qwen3-1.7b** on the other hand easily avoid the mistake mentioned above. But still, **both models** have no memory of past question.  \n",
    "\n",
    "I will treat this as future direction for improvement. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
